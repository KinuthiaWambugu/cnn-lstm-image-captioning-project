{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "976a3bc0",
   "metadata": {},
   "source": [
    "# Image Captioning — Testing / Inference Notebook (Saved Weights)\n",
    "\n",
    "This notebook recreates the **same model architecture** as the training notebook and then:\n",
    "\n",
    "1. Loads the saved **vocabulary** (so token IDs match).\n",
    "2. **Builds** the model once on dummy data.\n",
    "3. Loads your saved **weights** (`.h5`).\n",
    "4. Generates captions for new images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "91446300",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026-02-02 10:31:40.877113: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2026-02-02 10:31:41.959348: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2026-02-02 10:31:44.963989: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "/home/dministrator-drew/projects/cesi/Data Science Option/.venv/lib/python3.12/site-packages/keras/src/export/tf2onnx_lib.py:8: FutureWarning: In the future `np.object` will be defined as the corresponding NumPy scalar.\n",
      "  if not hasattr(np, \"object\"):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TF version: 2.20.0\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow.keras.preprocessing import image\n",
    "from tensorflow.keras.applications.inception_v3 import preprocess_input\n",
    "\n",
    "print(\"TF version:\", tf.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4abfb176",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Weights exists: True model.weights.h5\n",
      "Vocab exists: True vocab.json\n"
     ]
    }
   ],
   "source": [
    "WEIGHTS_PATH = r\"model.weights.h5\"\n",
    "VOCAB_PATH   = r\"vocab.json\"\n",
    "\n",
    "# Quick sanity checks:\n",
    "print(\"Weights exists:\", os.path.exists(WEIGHTS_PATH), WEIGHTS_PATH)\n",
    "print(\"Vocab exists:\", os.path.exists(VOCAB_PATH), VOCAB_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b709472d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#parameters\n",
    "MAX_LENGTH = 40\n",
    "VOCABULARY_SIZE = 15000\n",
    "EMBEDDING_DIM = 512\n",
    "UNITS = 512"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3c4446a",
   "metadata": {},
   "source": [
    "## Rebuild Encoder and Decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "36c63617",
   "metadata": {},
   "outputs": [],
   "source": [
    "def CNN_Encoder():\n",
    "    inception_v3 = tf.keras.applications.InceptionV3(\n",
    "        include_top = False,\n",
    "        weights = 'imagenet'\n",
    "    )\n",
    "\n",
    "    output = inception_v3.output\n",
    "    output = tf.keras.layers.Reshape((-1, output,shape[-1]))(output)\n",
    "\n",
    "    model = tf.keras.models.Model(inception_v3.input, output)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9e2250a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNN_Decoder(tf.keras.Model):\n",
    "    def __init__(self, embedding_dim, units, vocab_size):\n",
    "        super().__init__()\n",
    "        self.units = units\n",
    "        self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim)\n",
    "        self.gru = tf.keras.layers.GRU(\n",
    "            units,\n",
    "            return_sequences = True,\n",
    "            return_state = True\n",
    "        )\n",
    "        self.fc1 = tf.keras.layers.Dense(units)\n",
    "        self.fc2 = tf.keras.layers.Dense(vocab_size)\n",
    "\n",
    "    def call(self, x, features, hidden):\n",
    "        x = self.embedding(x)\n",
    "        x = tf.concat([tf.expand_dims(features, 1), x], axis=-1)\n",
    "        output, state = self.gru(x, initial_state=hidden)\n",
    "        x = self.fc1(output)\n",
    "        x = self.fc2(x)\n",
    "        return x, state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fee166c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocab size loaded: 11839\n",
      "First 20 tokens: ['', '[UNK]', 'a', '[start]', '[end]', 'on', 'of', 'the', 'in', 'with', 'and', 'is', 'man', 'to', 'sitting', 'an', 'two', 'at', 'people', 'standing']\n",
      "tokenizer.vocabulary_size(): 11839\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "I0000 00:00:1769761747.072572   28015 gpu_device.cc:2020] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 3584 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 3060 Laptop GPU, pci bus id: 0000:01:00.0, compute capability: 8.6\n"
     ]
    }
   ],
   "source": [
    "#load vocab and rebuild tokenizer+ lookup\n",
    "def load_vocab(vocab_path: str):\n",
    "    # If you saved JSON: [\"[pad]\", \"[start]\", ...]\n",
    "    if vocab_path.endswith(\".json\"):\n",
    "        with open(vocab_path, \"r\", encoding=\"utf-8\") as f:\n",
    "            vocab = json.load(f)\n",
    "        if not isinstance(vocab, list):\n",
    "            raise ValueError(\"vocab.json should contain a JSON list of tokens.\")\n",
    "        return vocab\n",
    "\n",
    "    # If you saved with pickle (like vocab_coco.file)\n",
    "    if vocab_path.endswith(\".file\") or vocab_path.endswith(\".pkl\") or vocab_path.endswith(\".pickle\"):\n",
    "        import pickle\n",
    "        with open(vocab_path, \"rb\") as f:\n",
    "            vocab = pickle.load(f)\n",
    "        return vocab\n",
    "\n",
    "    raise ValueError(\"Unknown vocab format. Use vocab.json or vocab_coco.file (pickle).\")\n",
    "\n",
    "vocab = load_vocab(VOCAB_PATH)\n",
    "print(\"Vocab size loaded:\", len(vocab))\n",
    "print(\"First 20 tokens:\", vocab[:20])\n",
    "\n",
    "# Rebuild tokenizer so it maps words -> ids exactly the same way.\n",
    "tokenizer = tf.keras.layers.TextVectorization(\n",
    "    max_tokens=VOCABULARY_SIZE,\n",
    "    standardize=None,\n",
    "    output_sequence_length=MAX_LENGTH,\n",
    ")\n",
    "\n",
    "# IMPORTANT:\n",
    "# We DO NOT call tokenizer.adapt(...) in inference.\n",
    "# Instead we set the vocabulary we loaded from training.\n",
    "tokenizer.set_vocabulary(vocab)\n",
    "\n",
    "# id <-> word helpers (same as your training notebook)\n",
    "word2idx = tf.keras.layers.StringLookup(\n",
    "    mask_token=\"\",\n",
    "    vocabulary=tokenizer.get_vocabulary()\n",
    ")\n",
    "\n",
    "idx2word = tf.keras.layers.StringLookup(\n",
    "    mask_token=\"\",\n",
    "    vocabulary=tokenizer.get_vocabulary(),\n",
    "    invert=True\n",
    ")\n",
    "\n",
    "print(\"tokenizer.vocabulary_size():\", tokenizer.vocabulary_size())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "abb2cc7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#model architecture\n",
    "def CNN_Encoder():\n",
    "    inception_v3 = tf.keras.applications.InceptionV3(\n",
    "        include_top=False,\n",
    "        weights=\"imagenet\"\n",
    "    )\n",
    "    output = inception_v3.output\n",
    "    output = tf.keras.layers.Reshape((-1, output.shape[-1]))(output)\n",
    "    cnn_model = tf.keras.models.Model(inception_v3.input, output)\n",
    "    return cnn_model\n",
    "\n",
    "\n",
    "class TransformerEncoderLayer(tf.keras.layers.Layer):\n",
    "    def __init__(self, embed_dim, num_heads):\n",
    "        super().__init__()\n",
    "        self.layer_norm_1 = tf.keras.layers.LayerNormalization()\n",
    "        self.layer_norm_2 = tf.keras.layers.LayerNormalization()\n",
    "        self.attention = tf.keras.layers.MultiHeadAttention(\n",
    "            num_heads=num_heads, key_dim=embed_dim\n",
    "        )\n",
    "        self.dense = tf.keras.layers.Dense(embed_dim, activation=\"relu\")\n",
    "\n",
    "    def call(self, x, training=False):\n",
    "        x = self.layer_norm_1(x)\n",
    "        x = self.dense(x)\n",
    "        attn_output = self.attention(\n",
    "            query=x, value=x, key=x,\n",
    "            attention_mask=None,\n",
    "            training=training\n",
    "        )\n",
    "        x = self.layer_norm_2(x + attn_output)\n",
    "        return x\n",
    "\n",
    "\n",
    "class Embeddings(tf.keras.layers.Layer):\n",
    "    def __init__(self, vocab_size, embed_dim, max_len):\n",
    "        super().__init__()\n",
    "        self.token_embeddings = tf.keras.layers.Embedding(vocab_size, embed_dim)\n",
    "        self.position_embeddings = tf.keras.layers.Embedding(max_len, embed_dim, input_shape=(None, max_len))\n",
    "\n",
    "    def call(self, input_ids):\n",
    "        length = tf.shape(input_ids)[-1]\n",
    "        position_ids = tf.range(start=0, limit=length, delta=1)\n",
    "        position_embeddings = self.position_embeddings(position_ids)\n",
    "        token_embeddings = self.token_embeddings(input_ids)\n",
    "        return token_embeddings + position_embeddings\n",
    "\n",
    "\n",
    "class TransformerDecoderLayer(tf.keras.layers.Layer):\n",
    "    def __init__(self, embed_dim, units, num_heads):\n",
    "        super().__init__()\n",
    "        self.embedding = Embeddings(tokenizer.vocabulary_size(), embed_dim, MAX_LENGTH)\n",
    "\n",
    "        self.attention_1 = tf.keras.layers.MultiHeadAttention(\n",
    "            num_heads=num_heads, key_dim=embed_dim, dropout=0.1\n",
    "        )\n",
    "        self.attention_2 = tf.keras.layers.MultiHeadAttention(\n",
    "            num_heads=num_heads, key_dim=embed_dim, dropout=0.1\n",
    "        )\n",
    "\n",
    "        self.layer_norm_1 = tf.keras.layers.LayerNormalization()\n",
    "        self.layer_norm_2 = tf.keras.layers.LayerNormalization()\n",
    "        self.layer_norm_3 = tf.keras.layers.LayerNormalization()\n",
    "\n",
    "        self.ffn1 = tf.keras.layers.Dense(units, activation=\"relu\")\n",
    "        self.ffn2 = tf.keras.layers.Dense(embed_dim)\n",
    "\n",
    "        self.dropout_1 = tf.keras.layers.Dropout(0.3)\n",
    "        self.dropout_2 = tf.keras.layers.Dropout(0.5)\n",
    "        self.dropout_3 = tf.keras.layers.Dropout(0.1)\n",
    "\n",
    "        self.out = tf.keras.layers.Dense(tokenizer.vocabulary_size(), activation=\"softmax\")\n",
    "\n",
    "    def get_causal_attention_mask(self, inputs):\n",
    "        input_shape = tf.shape(inputs)\n",
    "        batch_size, seq_len = input_shape[0], input_shape[1]\n",
    "        i = tf.range(seq_len)[:, None]\n",
    "        j = tf.range(seq_len)\n",
    "        mask = tf.cast(i >= j, dtype=\"int32\")\n",
    "        mask = tf.reshape(mask, (1, seq_len, seq_len))\n",
    "        mult = tf.concat(\n",
    "            [tf.expand_dims(batch_size, -1), tf.constant([1, 1], dtype=tf.int32)],\n",
    "            axis=0\n",
    "        )\n",
    "        return tf.tile(mask, mult)\n",
    "\n",
    "    def call(self, inputs, encoder_outputs, training=False, mask=None):\n",
    "        # inputs: (batch, seq_len)\n",
    "        # encoder_outputs: (batch, image_seq, embed_dim)\n",
    "        x = self.embedding(inputs)\n",
    "\n",
    "        causal_mask = self.get_causal_attention_mask(x)\n",
    "        if mask is not None:\n",
    "            # mask: (batch, seq_len) -> (batch, 1, seq_len)\n",
    "            padding_mask = tf.cast(mask[:, tf.newaxis, :], dtype=\"int32\")\n",
    "            combined_mask = tf.minimum(padding_mask, causal_mask)\n",
    "        else:\n",
    "            combined_mask = causal_mask\n",
    "\n",
    "        attn_1 = self.attention_1(\n",
    "            query=x, value=x, key=x,\n",
    "            attention_mask=combined_mask,\n",
    "            training=training\n",
    "        )\n",
    "        x = self.layer_norm_1(x + attn_1)\n",
    "\n",
    "        attn_2 = self.attention_2(\n",
    "            query=x, value=encoder_outputs, key=encoder_outputs,\n",
    "            attention_mask=padding_mask if mask is not None else None,\n",
    "            training=training\n",
    "        )\n",
    "        x = self.layer_norm_2(x + attn_2)\n",
    "\n",
    "        ffn_out = self.ffn1(x)\n",
    "        ffn_out = self.dropout_1(ffn_out, training=training)\n",
    "        ffn_out = self.ffn2(ffn_out)\n",
    "        x = self.layer_norm_3(x + ffn_out)\n",
    "\n",
    "        x = self.dropout_3(x, training=training)\n",
    "        return self.out(x)\n",
    "\n",
    "\n",
    "class ImageCaptioningModel(tf.keras.Model):\n",
    "    def __init__(self, cnn_model, encoder, decoder, image_aug=None):\n",
    "        super().__init__()\n",
    "        self.cnn_model = cnn_model\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "        self.image_aug = image_aug\n",
    "        self.loss_tracker = tf.keras.metrics.Mean(name=\"loss\")\n",
    "        self.acc_tracker = tf.keras.metrics.Mean(name=\"accuracy\")\n",
    "\n",
    "    def calculate_loss(self, y_true, y_pred, mask):\n",
    "        loss = self.loss(y_true, y_pred)\n",
    "        mask = tf.cast(mask, dtype=loss.dtype)\n",
    "        loss *= mask\n",
    "        return tf.reduce_sum(loss) / tf.reduce_sum(mask)\n",
    "\n",
    "    def calculate_accuracy(self, y_true, y_pred, mask):\n",
    "        accuracy = tf.equal(y_true, tf.argmax(y_pred, axis=2))\n",
    "        accuracy = tf.math.logical_and(mask, accuracy)\n",
    "        accuracy = tf.cast(accuracy, dtype=tf.float32)\n",
    "        mask = tf.cast(mask, dtype=tf.float32)\n",
    "        return tf.reduce_sum(accuracy) / tf.reduce_sum(mask)\n",
    "\n",
    "    def compute_loss_and_acc(self, img_embed, captions, training=True):\n",
    "        encoder_output = self.encoder(img_embed, training=training)\n",
    "        y_input = captions[:, :-1]\n",
    "        y_true = captions[:, 1:]\n",
    "        mask = (y_true != 0)\n",
    "        y_pred = self.decoder(y_input, encoder_output, training=training, mask=mask)\n",
    "        loss = self.calculate_loss(y_true, y_pred, mask)\n",
    "        acc = self.calculate_accuracy(y_true, y_pred, mask)\n",
    "        return loss, acc\n",
    "\n",
    "    def train_step(self, batch):\n",
    "        imgs, captions = batch\n",
    "        if self.image_aug:\n",
    "            imgs = self.image_aug(imgs)\n",
    "\n",
    "        img_embed = self.cnn_model(imgs)\n",
    "\n",
    "        with tf.GradientTape() as tape:\n",
    "            loss, acc = self.compute_loss_and_acc(img_embed, captions, training=True)\n",
    "\n",
    "        train_vars = self.encoder.trainable_variables + self.decoder.trainable_variables\n",
    "        grads = tape.gradient(loss, train_vars)\n",
    "        self.optimizer.apply_gradients(zip(grads, train_vars))\n",
    "\n",
    "        self.loss_tracker.update_state(loss)\n",
    "        self.acc_tracker.update_state(acc)\n",
    "        return {\"loss\": self.loss_tracker.result(), \"acc\": self.acc_tracker.result()}\n",
    "\n",
    "    def test_step(self, batch):\n",
    "        imgs, captions = batch\n",
    "        img_embed = self.cnn_model(imgs)\n",
    "        loss, acc = self.compute_loss_and_acc(img_embed, captions, training=False)\n",
    "        self.loss_tracker.update_state(loss)\n",
    "        self.acc_tracker.update_state(acc)\n",
    "        return {\"loss\": self.loss_tracker.result(), \"acc\": self.acc_tracker.result()}\n",
    "\n",
    "    @property\n",
    "    def metrics(self):\n",
    "        return [self.loss_tracker, self.acc_tracker]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a6a90ccc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/dministrator-drew/projects/cesi/Data Science Option/.venv/lib/python3.12/site-packages/keras/src/layers/core/embedding.py:103: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(**kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/inception_v3/inception_v3_weights_tf_dim_ordering_tf_kernels_notop.h5\n",
      "\u001b[1m87910968/87910968\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 0us/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026-01-30 09:30:07.017566: I external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:473] Loaded cuDNN version 91002\n",
      "2026-01-30 09:30:12.727630: W tensorflow/core/framework/op_kernel.cc:1842] INVALID_ARGUMENT: required broadcastable shapes\n",
      "2026-01-30 09:30:12.727689: I tensorflow/core/framework/local_rendezvous.cc:407] Local rendezvous is aborting with status: INVALID_ARGUMENT: required broadcastable shapes\n"
     ]
    },
    {
     "ename": "InvalidArgumentError",
     "evalue": "Exception encountered when calling Softmax.call().\n\n\u001b[1m{{function_node __wrapped__SelectV2_device_/job:localhost/replica:0/task:0/device:GPU:0}} required broadcastable shapes [Op:SelectV2] name: \u001b[0m\n\nArguments received by Softmax.call():\n  • inputs=tf.Tensor(shape=(1, 8, 39, 64), dtype=float32)\n  • mask=tf.Tensor(shape=(1, 1, 1, 39), dtype=bool)",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mInvalidArgumentError\u001b[39m                      Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[10]\u001b[39m\u001b[32m, line 18\u001b[39m\n\u001b[32m     16\u001b[39m \u001b[38;5;66;03m# Run 1 forward pass through the training logic (no gradients needed here)\u001b[39;00m\n\u001b[32m     17\u001b[39m img_embed = caption_model.cnn_model(dummy_imgs)\n\u001b[32m---> \u001b[39m\u001b[32m18\u001b[39m _ = \u001b[43mcaption_model\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcompute_loss_and_acc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimg_embed\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdummy_captions\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtraining\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[32m     20\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mBuilt encoder vars:\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28mlen\u001b[39m(caption_model.encoder.variables))\n\u001b[32m     21\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mBuilt decoder vars:\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28mlen\u001b[39m(caption_model.decoder.variables))\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[9]\u001b[39m\u001b[32m, line 151\u001b[39m, in \u001b[36mImageCaptioningModel.compute_loss_and_acc\u001b[39m\u001b[34m(self, img_embed, captions, training)\u001b[39m\n\u001b[32m    149\u001b[39m y_true = captions[:, \u001b[32m1\u001b[39m:]\n\u001b[32m    150\u001b[39m mask = (y_true != \u001b[32m0\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m151\u001b[39m y_pred = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mdecoder\u001b[49m\u001b[43m(\u001b[49m\u001b[43my_input\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mencoder_output\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtraining\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtraining\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmask\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    152\u001b[39m loss = \u001b[38;5;28mself\u001b[39m.calculate_loss(y_true, y_pred, mask)\n\u001b[32m    153\u001b[39m acc = \u001b[38;5;28mself\u001b[39m.calculate_accuracy(y_true, y_pred, mask)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/projects/cesi/Data Science Option/.venv/lib/python3.12/site-packages/keras/src/utils/traceback_utils.py:122\u001b[39m, in \u001b[36mfilter_traceback.<locals>.error_handler\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    119\u001b[39m     filtered_tb = _process_traceback_frames(e.__traceback__)\n\u001b[32m    120\u001b[39m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[32m    121\u001b[39m     \u001b[38;5;66;03m# `keras.config.disable_traceback_filtering()`\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m122\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m e.with_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    123\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m    124\u001b[39m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[9]\u001b[39m\u001b[32m, line 107\u001b[39m, in \u001b[36mTransformerDecoderLayer.call\u001b[39m\u001b[34m(self, inputs, encoder_outputs, training, mask)\u001b[39m\n\u001b[32m    100\u001b[39m attn_1 = \u001b[38;5;28mself\u001b[39m.attention_1(\n\u001b[32m    101\u001b[39m     query=x, value=x, key=x,\n\u001b[32m    102\u001b[39m     attention_mask=combined_mask,\n\u001b[32m    103\u001b[39m     training=training\n\u001b[32m    104\u001b[39m )\n\u001b[32m    105\u001b[39m x = \u001b[38;5;28mself\u001b[39m.layer_norm_1(x + attn_1)\n\u001b[32m--> \u001b[39m\u001b[32m107\u001b[39m attn_2 = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mattention_2\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    108\u001b[39m \u001b[43m    \u001b[49m\u001b[43mquery\u001b[49m\u001b[43m=\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m=\u001b[49m\u001b[43mencoder_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkey\u001b[49m\u001b[43m=\u001b[49m\u001b[43mencoder_outputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    109\u001b[39m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpadding_mask\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mmask\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mis\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    110\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtraining\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtraining\u001b[49m\n\u001b[32m    111\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    112\u001b[39m x = \u001b[38;5;28mself\u001b[39m.layer_norm_2(x + attn_2)\n\u001b[32m    114\u001b[39m ffn_out = \u001b[38;5;28mself\u001b[39m.ffn1(x)\n",
      "\u001b[31mInvalidArgumentError\u001b[39m: Exception encountered when calling Softmax.call().\n\n\u001b[1m{{function_node __wrapped__SelectV2_device_/job:localhost/replica:0/task:0/device:GPU:0}} required broadcastable shapes [Op:SelectV2] name: \u001b[0m\n\nArguments received by Softmax.call():\n  • inputs=tf.Tensor(shape=(1, 8, 39, 64), dtype=float32)\n  • mask=tf.Tensor(shape=(1, 1, 1, 39), dtype=bool)"
     ]
    }
   ],
   "source": [
    "encoder = TransformerEncoderLayer(EMBEDDING_DIM, num_heads=1)\n",
    "decoder = TransformerDecoderLayer(EMBEDDING_DIM, UNITS, num_heads=8)\n",
    "cnn_model = CNN_Encoder()\n",
    "\n",
    "caption_model = ImageCaptioningModel(cnn_model=cnn_model, encoder=encoder, decoder=decoder, image_aug=None)\n",
    "\n",
    "# compile so `self.loss` exists (required by compute_loss_and_acc)\n",
    "cross_entropy = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=False, reduction=\"none\")\n",
    "caption_model.compile(optimizer=tf.keras.optimizers.Adam(), loss=cross_entropy)\n",
    "\n",
    "# --- BUILD STEP (very important for subclassed models) ---\n",
    "# Create dummy inputs to force all variables to be created before load_weights()\n",
    "dummy_imgs = tf.zeros((1, 299, 299, 3), dtype=tf.float32)\n",
    "dummy_captions = tf.zeros((1, MAX_LENGTH), dtype=tf.int64)\n",
    "\n",
    "# Run 1 forward pass through the training logic (no gradients needed here)\n",
    "img_embed = caption_model.cnn_model(dummy_imgs)\n",
    "_ = caption_model.compute_loss_and_acc(img_embed, dummy_captions, training=False)\n",
    "\n",
    "print(\"Built encoder vars:\", len(caption_model.encoder.variables))\n",
    "print(\"Built decoder vars:\", len(caption_model.decoder.variables))\n",
    "\n",
    "# Now load weights safely\n",
    "caption_model.load_weights(WEIGHTS_PATH)\n",
    "print(\"✅ Weights loaded!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bae41e7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
